{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/stact/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask,render_template,request,redirect, url_for, send_from_directory\n",
    "from werkzeug import secure_filename\n",
    "import os\n",
    "from celery import Celery\n",
    "import redis\n",
    "from flask import jsonify\n",
    "import celery\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import requests\n",
    "import re # Regular expressions\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "from scipy import spatial\n",
    "from bokeh.layouts import row\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.embed import components\n",
    "import numpy as np\n",
    "from bokeh.layouts import row\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from collections import Counter\n",
    "from bokeh.io import hplot, output_file, show\n",
    "from docx import Document\n",
    "import re\n",
    "import math\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "app_lulu = Flask(__name__, static_folder = 'static',static_url_path = '/static')\n",
    "\n",
    "# This is the path to the upload directory a sql server?\n",
    "app_lulu.config['UPLOAD_FOLDER'] = 'uploads/'\n",
    "\n",
    "# These are the extension that we are accepting to be uploaded\n",
    "app_lulu.config['ALLOWED_EXTENSIONS'] = set(['docx'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_lulu.config['CELERY_BROKER_URL'] = 'redis://127.0.0.1:6379/0'\n",
    "\n",
    "#Have Celery store status and results from tasks\n",
    "app_lulu.config['CELERY_RESULT_BACKEND'] = 'redis://127.0.0.1:6379/0'\n",
    "\n",
    "#Tell Celery where the broker service is running\n",
    "celery = Celery(app_lulu.name, broker=app_lulu.config['CELERY_BROKER_URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getText(filename):\n",
    "    doc = Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "\n",
    "    resume = '\\n'.join(fullText)\n",
    "    lines = [line.strip() for line in resume.splitlines()]\n",
    "    chunks = [re.split('; |, |\\*|\\(|\\)|\\:| ',line) for line in lines]\n",
    "    texts = []\n",
    "    for chunk in chunks:\n",
    "        texts = texts + chunk\n",
    "    text_clean = [text.lower() for text in texts]\n",
    "    doc_frequency = Counter() # This will create a full counter of our terms.\n",
    "    doc_frequency.update(text_clean) # List comp\n",
    "    my = Counter({'R':doc_frequency['r'], 'Python':doc_frequency['python'],\n",
    "                 'Java':doc_frequency['java'], 'C++':doc_frequency['c++'],\n",
    "                 'Ruby':doc_frequency['ruby'], 'Perl':doc_frequency['perl'],\n",
    "                 'Matlab':doc_frequency['matlab'], 'JavaScript':doc_frequency['javascript'],\n",
    "                 'Scala': doc_frequency['scala'],'Excel':doc_frequency['excel'],\n",
    "                 'Tableau':doc_frequency['tableau'], 'D3.js':doc_frequency['d3.js'],\n",
    "                 'SAS':doc_frequency['sas'], 'SPSS':doc_frequency['spss'],\n",
    "                 'D3':doc_frequency['d3'], 'Hadoop':doc_frequency['hadoop'],\n",
    "                 'MapReduce':doc_frequency['mapreduce'], 'Spark':doc_frequency['spark'],\n",
    "                 'Pig':doc_frequency['pig'], 'Hive':doc_frequency['hive'],\n",
    "                 'Shark':doc_frequency['shark'], 'Oozie':doc_frequency['oozie'],\n",
    "                 'ZooKeeper':doc_frequency['zookeeper'], 'Flume':doc_frequency['flume'],\n",
    "                 'Mahout':doc_frequency['mahout'], 'SQL':doc_frequency['sql'],\n",
    "                 'NoSQL':doc_frequency['nosql'],'HBase':doc_frequency['hbase'],\n",
    "                 'Cassandra':doc_frequency['cassandra'],'MongoDB':doc_frequency['mongodb']})\n",
    "    my_df = pd.DataFrame(my,index=[0])\n",
    "    my_df[my_df > 0]=1\n",
    "    return my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_df = getText('resume_Xiang_Li.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_skills(website):\n",
    "    '''\n",
    "        This function just cleans up the raw html so that I can look at it.\n",
    "        Inputs: a URL to investigate\n",
    "        Outputs: Cleaned text only\n",
    "        '''\n",
    "    try:\n",
    "        session_requests=requests.session()\n",
    "        site=session_requests.get(website).content # Connect to the job posting\n",
    "    except:\n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem\n",
    "\n",
    "    soup_obj = BeautifulSoup(site) # Get the html from the site\n",
    "\n",
    "    if len(soup_obj) == 0: # In case the default parser lxml doesn't work, try another one\n",
    "        soup_obj = BeautifulSoup(site, 'html5lib')\n",
    "\n",
    "\n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "\n",
    "    text = soup_obj.get_text() # Get the text from this\n",
    "    lines = [line.strip() for line in text.splitlines()] # break into lines\n",
    "    chunks = [phrase.strip() for line in lines for phrase in line.split(\"  \")] # break multi-headlines into a line each\n",
    "    text = ''.join(chunk for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "\n",
    "    # Now clean out all of the unicode junk (this line works great!!!)\n",
    "    try:\n",
    "        text = text.decode(encoding = 'utf-8') # Need this as some websites aren't formatted\n",
    "    except:                                                            # in a way that this works, can occasionally throw\n",
    "        return                                                         # an exception\n",
    "\n",
    "    text = re.sub(\"[^a-zA-Z+3]\",\" \", text)  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "    # Also include + for C++\n",
    "    text = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text) # Fix spacing issue from merged words\n",
    "    text = text.lower().split()  # Go to lower case and split them apart\n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "\n",
    "    text = list(set(text)) # Last, just get the set of these. Ignore counts (we are just looking at whether a term existed\n",
    "    # or not on the website)\n",
    "    doc_frequency = Counter() # This will create a full counter of our terms.\n",
    "    doc_frequency.update(text) # List comp\n",
    "    #create dictionaries indicating skill set\n",
    "    prog_lang_dict = Counter({'R':doc_frequency['r'], 'Python':doc_frequency['python'],\n",
    "                             'Java':doc_frequency['java'], 'C++':doc_frequency['c++'],\n",
    "                             'Ruby':doc_frequency['ruby'],\n",
    "                             'Perl':doc_frequency['perl'], 'Matlab':doc_frequency['matlab'],\n",
    "                             'JavaScript':doc_frequency['javascript'], 'Scala': doc_frequency['scala']})\n",
    "\n",
    "    analysis_tool_dict = Counter({'Excel':doc_frequency['excel'],  'Tableau':doc_frequency['tableau'],\n",
    "                             'D3.js':doc_frequency['d3.js'], 'SAS':doc_frequency['sas'],\n",
    "                             'SPSS':doc_frequency['spss'], 'D3':doc_frequency['d3']})\n",
    "\n",
    "    hadoop_dict = Counter({'Hadoop':doc_frequency['hadoop'], 'MapReduce':doc_frequency['mapreduce'],\n",
    "                          'Spark':doc_frequency['spark'], 'Pig':doc_frequency['pig'],\n",
    "                          'Hive':doc_frequency['hive'], 'Shark':doc_frequency['shark'],\n",
    "                          'Oozie':doc_frequency['oozie'], 'ZooKeeper':doc_frequency['zookeeper'],\n",
    "                          'Flume':doc_frequency['flume'], 'Mahout':doc_frequency['mahout']})\n",
    "\n",
    "    database_dict = Counter({'SQL':doc_frequency['sql'], 'NoSQL':doc_frequency['nosql'],\n",
    "                                                  'HBase':doc_frequency['hbase'], 'Cassandra':doc_frequency['cassandra'],\n",
    "                                                  'MongoDB':doc_frequency['mongodb']})\n",
    "    post_skills = prog_lang_dict.copy()\n",
    "    post_skills.update(analysis_tool_dict)\n",
    "    post_skills.update(hadoop_dict)\n",
    "    post_skills.update(database_dict)\n",
    "    post_df = pd.DataFrame(dict(post_skills),index = [0])\n",
    "\n",
    "    return post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@celery.task(bind=True)\n",
    "def post_info(self, city = None, state =None):\n",
    "\n",
    "    final_job = 'data+scientist'\n",
    "\n",
    "    if city is not None:\n",
    "        final_city = city.split()\n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=%22', final_job, '%22&l=', final_city,\n",
    "                           '%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "    else:\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "\n",
    "    final_site = ''.join(final_site_list)\n",
    "    base_url = 'http://www.indeed.com'\n",
    "\n",
    "    try:\n",
    "        session_requests=requests.session()\n",
    "        html =session_requests.get(final_site).content #open the job search page\n",
    "    except:\n",
    "        'That city/state combination did not have any jobs. Exiting'\n",
    "        return\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    #Now find out how many jobs there were\n",
    "\n",
    "    num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8')\n",
    "    job_numbers = re.findall('\\d+', str(num_jobs_area))\n",
    "\n",
    "    if len(job_numbers) > 3:#Have a total number of jobs greater than 1000?\n",
    "        total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "    else:\n",
    "        total_num_jobs = int(job_numbers[2])\n",
    "    city_title = city\n",
    "    if city is None:\n",
    "        city_title = 'Nationawide'\n",
    "    print('There were', total_num_jobs, 'jobs found', city_title)\n",
    "\n",
    "    num_pages = total_num_jobs/10 #know the total number of time we attempt search result page\n",
    "\n",
    "    rank = []\n",
    "    for i in range(1, 2):#int(np.ceil(num_pages)+1)\n",
    "        print('Getting page',i)\n",
    "        start_num = str(i*10)\n",
    "        current_page = ''.join([final_site,'&start=',start_num])\n",
    "        session_requests=requests.session()\n",
    "        html_page =session_requests.get(current_page).content #open the job search page\n",
    "        page_obj = BeautifulSoup(html_page)\n",
    "        job_link_area = page_obj.find(id = 'resultsCol') #Only the center area of the page\n",
    "\n",
    "        job_URLS = [base_url + str(link.get('href')) for link in job_link_area.findAll('a')]\n",
    "        job_URLS = list(filter(lambda x: 'clk' in x, job_URLS))\n",
    "        \n",
    "        self.update_state(state='PROGRESS',\n",
    "                          meta={'current': i, 'total': int(np.ceil(num_pages))})\n",
    "\n",
    "        for j in range(0, len(job_URLS)):\n",
    "            final_description = get_skills(job_URLS[j])\n",
    "            score = round(1 - spatial.distance.cosine(my_df,final_description),2)\n",
    "            if math.isnan(score) is False:    #need to address nan website later, two situation: different formats, no skill match\n",
    "                rank.append([job_URLS[j],float(score)])\n",
    "            \n",
    "        sleep(1)\n",
    "    rank = sorted(rank, key = lambda x: x[1], reverse = True)\n",
    "    print('Done collecting job posts')\n",
    "    return{'current':100, 'total':100, 'status':'Task completed!', 'result':42}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = post_info(city = 'Boston', state = 'MA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task = post_info.apply_async(args=[\"Boston\", \"MA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task_id = task.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'54d49e81-6c10-4777-8414-5fbc55ccaa27'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Working outside of application context.\n\nThis typically means that you attempted to use functionality that needed\nto interface with the current application object in a way.  To solve\nthis set up an application context with app.app_context().  See the\ndocumentation for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-faacb9140191>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjsonify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m202\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Location'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'taskstatus'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/stact/anaconda/lib/python3.5/site-packages/flask/json.py\u001b[0m in \u001b[0;36mjsonify\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0mseparators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcurrent_app\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'JSONIFY_PRETTYPRINT_REGULAR'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_xhr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mindent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mseparators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m': '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stact/anaconda/lib/python3.5/site-packages/werkzeug/local.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__members__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_current_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_current_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stact/anaconda/lib/python3.5/site-packages/werkzeug/local.py\u001b[0m in \u001b[0;36m_get_current_object\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \"\"\"\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__release_local__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stact/anaconda/lib/python3.5/site-packages/flask/globals.py\u001b[0m in \u001b[0;36m_find_app\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_app_ctx_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_app_ctx_err_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Working outside of application context.\n\nThis typically means that you attempted to use functionality that needed\nto interface with the current application object in a way.  To solve\nthis set up an application context with app.app_context().  See the\ndocumentation for more information."
     ]
    }
   ],
   "source": [
    "jsonify({}), 202, {'Location': url_for('taskstatus',task_id=task.id)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task = post_info.AsyncResult(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DisabledBackend' object has no attribute '_get_task_meta_for'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e7dfef41d70e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/stact/anaconda/lib/python3.5/site-packages/celery/result.py\u001b[0m in \u001b[0;36mstate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0mthen\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtasks\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \"\"\"\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_task_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m  \u001b[0;31m# XXX compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stact/anaconda/lib/python3.5/site-packages/celery/result.py\u001b[0m in \u001b[0;36m_get_task_meta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_task_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_set_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_task_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stact/anaconda/lib/python3.5/site-packages/celery/backends/base.py\u001b[0m in \u001b[0;36mget_task_meta\u001b[0;34m(self, task_id, cache)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_task_meta_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DisabledBackend' object has no attribute '_get_task_meta_for'"
     ]
    }
   ],
   "source": [
    "task.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_job = 'data+scientist'\n",
    "city = 'Boston'\n",
    "state = 'MA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if city is not None:\n",
    "        final_city = city.split()\n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=%22', final_job, '%22&l=', final_city,\n",
    "                           '%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "else:\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "\n",
    "final_site = ''.join(final_site_list)\n",
    "base_url = 'http://www.indeed.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session_requests=requests.session()\n",
    "html =session_requests.get(final_site).content #open the job search pag\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "1 2 3 4 10 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import functools\n",
    "\n",
    "def add(x,y):\n",
    "    return x+y\n",
    "\n",
    "n = int(input().strip())\n",
    "arr = [int(arr_temp) for arr_temp in input().strip().split(' ')]\n",
    "\n",
    "functools.reduce(add, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 5 6 \n",
      "1 2 3 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0,a1,a2 = input().strip().split(' ')\n",
    "a = [int(a0),int(a1),int(a2)]\n",
    "b0,b1,b2 = input().strip().split(' ')\n",
    "b = [int(b0),int(b1),int(b2)]\n",
    "\n",
    "\n",
    "[1 for i in range(3) if a[i]>b[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = int(input().strip())\n",
    "if n<0|n>10:\n",
    "    print(\"Print enter a correct number\")\n",
    "    n = int(input().strip())\n",
    "arr = [int(arr_temp) for arr_temp in input().strip().split(' ')]\n",
    "\n",
    "print(sum(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
